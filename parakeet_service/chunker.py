"""
Offline VAD-aware splitters
───────────────────────────
* vad_chunk_lowmem     – Low-memory chunker for non-streaming processing
  - target 60-s chunks (±10 s)
  - never cut mid-utterance (trailing silence ≥ 300 ms)
  - processes audio incrementally to minimize memory usage
  - returns List[pathlib.Path] of temp .wav files

* vad_chunk_streaming  – Low-RAM streamer for streaming use cases
  - processes audio in small stripes (2 seconds at a time)
  - uses VADIterator to split on speech boundaries
  - returns List[pathlib.Path] of temp .wav files
"""

from __future__ import annotations
import tempfile, wave, pathlib, numpy as np
from typing import List
import soundfile as sf

# Note: Still using torch for VAD as it's the most reliable option available
try:
    from torch.hub import load as torch_hub_load

    vad_model, vad_utils = torch_hub_load("snakers4/silero-vad", "silero_vad")
    get_speech_ts, _, _, VADIterator, _ = vad_utils
    VAD_AVAILABLE = True
except ImportError:
    VAD_AVAILABLE = False
    print("Warning: torch not available, chunking will use simple time-based splitting")

SAMPLE_RATE = 16_000
TARGET_SEC = 60
MAX_SEC = 70  # never exceed this in one chunk
TRAIL_SIL_MS = 300  # keep ≥300 ms silence at cut point
THRESH = 0.60  # stricter prob threshold


def vad_chunk_lowmem(path: pathlib.Path) -> List[pathlib.Path]:
    """Low-memory VAD chunking for non-streaming processing"""
    if not VAD_AVAILABLE:
        return _simple_time_chunk(path)

    import librosa

    # Get audio file info
    with sf.SoundFile(path) as snd:
        file_sr = snd.samplerate
        duration = len(snd) / file_sr

    # Initialize VAD iterator
    vad_iter = VADIterator(
        vad_model,
        sampling_rate=SAMPLE_RATE,
        threshold=THRESH,
        min_silence_duration_ms=TRAIL_SIL_MS,
        speech_pad_ms=SPEECH_PAD_MS,
    )

    # Buffer for current chunk
    current_chunk = bytearray()
    chunks = []
    speech_ms = 0

    # Process audio in chunks
    for chunk_start in range(0, int(duration * SAMPLE_RATE), STRIPE_FRAMES):
        # Load audio segment with librosa for format conversion
        y, _ = librosa.load(
            path,
            sr=SAMPLE_RATE,
            offset=chunk_start / SAMPLE_RATE,
            duration=STRIPE_SEC,
            mono=True,
        )

        # Convert to int16
        audio_int16 = (y * 32767).astype(np.int16)

        # Process in 512-sample windows
        for i in range(0, len(audio_int16), 512):
            window = audio_int16[i : i + 512]
            if len(window) < 512:
                break

            # Convert to float for VAD
            window_f32 = window.astype(np.float32) / 32768
            evt = vad_iter(window_f32)

            # Add to current chunk
            current_chunk.extend(window.tobytes())
            speech_ms += 32

            # Check if we should finalize chunk
            if (evt and evt.get("end")) or speech_ms >= MAX_CHUNK_MS:
                if current_chunk:
                    chunks.append(_flush(current_chunk))
                    current_chunk.clear()
                    speech_ms = 0

    # Finalize last chunk
    if current_chunk:
        chunks.append(_flush(current_chunk))

    return chunks


def _simple_time_chunk(path: pathlib.Path) -> List[pathlib.Path]:
    """Simple time-based chunking when VAD is not available"""
    chunks = []

    with sf.SoundFile(path) as snd:
        duration = len(snd) / snd.samplerate
        chunk_duration = TARGET_SEC

        for start_time in range(0, int(duration), chunk_duration):
            end_time = min(start_time + chunk_duration, duration)

            # Read chunk
            snd.seek(int(start_time * snd.samplerate))
            chunk_samples = int((end_time - start_time) * snd.samplerate)
            audio_data = snd.read(chunk_samples, dtype="int16")

            # Save chunk
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            with wave.open(tmp, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(SAMPLE_RATE)
                wf.writeframes(audio_data.tobytes())
            chunks.append(pathlib.Path(tmp.name))

    return chunks


# Constants for both chunkers
STRIPE_SEC = 2  # read 2-second stripes
STRIPE_FRAMES = SAMPLE_RATE * STRIPE_SEC
MAX_CHUNK_MS = 60_000  # hard 60s cap
SPEECH_PAD_MS = 120  # same as live VAD
TARGET_SEC = 60
MAX_SEC = 70
TRAIL_SIL_MS = 300
THRESH = 0.60


def _flush(buf: bytearray) -> pathlib.Path:
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    with wave.open(tmp, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(SAMPLE_RATE)
        wf.writeframes(bytes(buf))
    return pathlib.Path(tmp.name)


def vad_chunk_streaming(path: pathlib.Path) -> List[pathlib.Path]:
    """
    Stream the file in small stripes and split on VADIterator boundaries.
    Uses the SAME PyTorch Silero model, but keeps only a few seconds in RAM.
    """
    if not VAD_AVAILABLE:
        return _simple_time_chunk(path)

    vad_iter = VADIterator(
        vad_model,
        sampling_rate=SAMPLE_RATE,
        threshold=THRESH,
        min_silence_duration_ms=TRAIL_SIL_MS,
        speech_pad_ms=SPEECH_PAD_MS,
    )

    paths, buf = [], bytearray()
    speech_ms = 0

    with sf.SoundFile(path) as snd:
        while True:
            audio = snd.read(frames=STRIPE_FRAMES, dtype="int16", always_2d=False)
            if not len(audio):
                break

            # Normalise to float32 [-1,1] for VADIterator
            audio_f32 = audio.astype("float32") / 32768

            # Feed 512-sample windows
            for start in range(0, len(audio_f32), 512):
                window = audio_f32[start : start + 512]
                if len(window) < 512:
                    break
                evt = vad_iter(window)
                buf.extend(audio[start : start + 512].tobytes())
                speech_ms += 32

                if (evt and evt.get("end")) or speech_ms >= MAX_CHUNK_MS:
                    paths.append(_flush(buf))
                    buf.clear()
                    speech_ms = 0

    if buf:
        paths.append(_flush(buf))
    return paths
